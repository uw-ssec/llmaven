app_id,question,true_answer,context,RAG_generated_answer,trulens_Answer_Relevance,trulens_Groundedness,trulens_Context_Relevance
app_hash_249cdbf218ae5cad989db0fbaf09493b,"Hi, 
I‚Äôm following this tutorial:  The LSST Science Pipelines ‚Äî LSST Science Pipelines  and I‚Äôve ran the first step ‚Äúsingle_frame‚Äù task a few times. Each time it runs it produces different results: if I go through all calexps in the output collection ( butler.registry.queryDatasets(""calexp"", collections=collection) ), and look at their sky coverage (calexp width, height and WCS mapping), and then find the total coverage of the whole collection (max and min ra, dec coordinates), I get different results each time it runs. And I‚Äôm starting it like this (verbatim what‚Äôs in the tutorial): 
 pipetask run -b $RC2_SUBSET_DIR/SMALL_HSC/butler.yaml \
             -p $RC2_SUBSET_DIR/pipelines/DRP.yaml#singleFrame \
             -i HSC/RC2/defaults \
             -o u/$USER/single_frame \
             --register-dataset-types
 
 What could be the explanation for this behavior? 
 Thanks, 
Petar","Quick comment on the code: 
 
 
 
  petarz: 
 
 calexp = butler.get('calexp', dataId=ref.dataId.full, collections=in_collection) 
 
 
 Use  calexp = butler.getDirect(ref)  to get the actual dataset that the query has returned. Otherwise butler does a whole new query and so will very likely not return the thing that your ref is really associated with (because the query will return all matching datasets in those collections but the  .get()  will return the first match in the given collections).  (you also should not need to use  ref.dataid.full  ‚Äì  ref.dataid  should be sufficient). 
 
 
 
  petarz: 
 
 mjd = calexp.getMetadata().toDict()['MJD'] 
 
 
 The  toDict  is not needed.  mjd = calexp.getMetadata()[""MJD""]  has worked for a few years now. 
 
 
 
  petarz: 
 
 w, h = calexp.width, calexp.height 
 
 
 You should use  getBBox()  to get the bounding box and then use the upper and lower bounds from that rather than assuming 0,0.","Draft
LVV-P106: Data Management Acceptance Test Campaign, Fall 2023 Test Plan and Report | DMTR-401 | Latest Revision 2024-03-25
Actual Result
Run the same query as before, but specifying the exact collection that the results were saved into:
butler query-datasets /repo/main calexp --collections u/jcarlin/LVV-T142_test/20231102T233609Z | grep ‚Äôcalexp‚Äô
| wc
240
1920 31680
We see that this execution brought the total to 240 calexps, as expected. We have thus demonstrated that the
batch processing system (BPS) at the USDF successfully recovers jobs that fail before they have completed.
5.1.3.11
LVV-T1748 - Verify calculation of median error in absolute position for RA, Dec
axes
Version 1. Status Approved. Open LVV-T1748 test case in Jira.
Verify that the DM system has provided the code to calculate the median error in absolute
position for each axis, RA and DEC, and assess whether it meets the requirement that it shall
be less than AA1 = 50 milliarcseconds.
Preconditions:
Execution status:
Final comment:
Detailed steps results LVV-C260-LVV-T1748 LVV-E2973-3368:
Note: Steps ‚ÄùNot Executed‚Äù and with No Result are not shown in this report.
Step LVV-E2973-1
Step Execution Status: Pass
Description
Identify a dataset containing processed data.
Expected Result
A dataset that has been ingested into a Butler repository.
D R A F T
49
D R A F T


LDM-GEN3: Gen 3 Butler Acceptance Testing Test Plan and Report | DMTR-271 | Latest Revision 2022-07-08
Examine the ‚Äòcalexp‚Äò and ‚Äòwcs‚Äò to conÔ¨Årm that they are diÔ¨Äerent:
In [6]: calexp
Out[6]: <lsst.afw.image.exposure.ExposureF at 0x7fc6d9b83130>
As expected, the calexp is an ExposureF object.
In [7]: wcs
Out[7]:
FITS standard SkyWcs:
Sky Origin: (149.8520271457, +2.0585702399)
Pixel Origin: (1003.05, 2415.24)
Pixel Scale: 0.16713 arcsec/pixel
The WCS looks like a properly deÔ¨Åned WCS. Now look at the image plane of the calexp:
In [8]: calexp.image
Out[8]:
lsst.afw.image.image.ImageF=[[ -0.36441362 -0.3609193 -0.35746038 ‚Ä¶ -25.336197 -25.346905
-25.357626 ]
[ -0.3578999 -0.354396 -0.3511718 ‚Ä¶ -25.327019 -25.337673
-25.348345 ]
[ -0.3513667 -0.34809738 -0.34461957 ‚Ä¶ -25.317785 -25.328388
-25.339252 ]
‚Ä¶
[ 28.878033 28.84473 28.811472 ‚Ä¶ 7.566758 7.5311904
7.4954834 ]
[ 28.914822 28.88162 28.848219 ‚Ä¶ 7.5671864 7.5316124
7.4958982 ]
[ 28.951662 28.918072 28.884766 ‚Ä¶ 7.567666 7.5320854
7.4963655 ]], bbox=(minimum=(0, 0), maximum=(2047, 4175))
These look good. We have thus demonstrated that the data products of Data Release Production performed on
the Batch Processing System at NCSA can be retrieved on the LSST development machines at NCSA.
5.3.3.4
LVV-T2499 - Verify Consistent Output Interface
51
","
Answer: Hi Petar! This behavior is expected and is not a problem with your setup. In fact, it's desirable since the purpose of running the task multiple times is to test its robustness in handling different input data.

The sky coverage (width, height, WCS mapping) and total coverage of the collection can vary each time the task runs because the input data has random elements within it. For example, the sky locations in the single_frame task are generated randomly based on certain rules and constraints. The task itself does not modify or change the original data, but rather processes them according to the specified workflow.

So you can expect different results when running the same task multiple times with the same inputs, as long as the task is configured correctly and has no bugs. This behavior ensures that the task works reliably and produces consistent outputs across different runs.

In summary, your observations are a good sign because they demonstrate that the LSST Science Pipelines are working as intended and can handle variations in input data. Just make sure that the task is set up correctly according to the tutorial and is configured with the appropriate settings for your use case. This will ensure consistent results each time you run the task.",1.0,0.1212121212121212,0.3333333333333333
app_hash_249cdbf218ae5cad989db0fbaf09493b,"I have the following C++ class : 
 class CcdImageList : public std::list  > 
The Swig object is correctly transmitted to the Python, but I cannot iterate on the list. 
What should I do (in Swig ? )  to make it iterable ?","After several iteration with  @ktl  and  @rowen  we finally solved this. The Swig interface should contain: 
 %include ""lsst/meas/simastrom/CountedRef.h""
%template(CcdImageCountedRef) lsst::meas::simastrom::CountedRef ;
%template(CcdImageCountedRefList) std::list  >;
%include ""lsst/meas/simastrom/CcdImage.h""
 
 In this order. The bottom line is that Swig is processing the .i file in one single pass, so every ‚Äúobject‚Äù should be known before being used.","In most cases, the SWIG files from the current stack will contain the necessary python code and
one can simply copy and paste the code from the SWIG file into the new python file with little
modification.
Frequently Encountered Problems
There are a number of errors, issues, and other problems that you are likely to come across
during wrapping. This section has some hints on what might be causing a particular problem you
are encountering.
Casting
SWIG and pybind11 handle inheritance in different ways. In SWIG, if a class B inherits from A, a
pointer that clones B can return a type A, which is undesirable. There was a lot of machinery,
including a .cast  method that was used to recase A as B. This is not necessary with pybind11 so
all casting procedures can be removed (or at the very least commented out) and tests for casting
can be skipped with a @unittest.skip(""Skip for pybind11"") .
Segmentation Faults
Smart Pointers
The vast majority of the segfaults you encounter will be caused by inheriting a class that is
defined with a smart pointer, but not using the same pointer in the template definition of the
new class (see smart_ptr). For example if a class A is defined using
py::class_<A, std::shared_ptr<A>> clsA(mod, ""A"");
then a class B that inherits from A must include std::shared_ptr<B> :
py::class_<B, std::shared_ptr<B>, A> clsB(mod, ""B"");
NDArrays
The other main cause of segfaults is forgetting to include
Loading [MathJax]/extensions/tex2jax.js


def _getitem_(self, index):
    """"""
    operator[] in C++ only returns a single item, but `Array` has a method to get a slice of the
    array. To make the code more python we automatically check for a slice and return either
    a single item or slice as requested by the user.
    """"""
    if isinstance(index, slice):
        start, stop, stride = index.indices(self.getSize())
        if stride != 1:
            raise IndexError(""Non-unit stride not supported"")
        return self.slice(start, stop)
    return self._get_(index)
which uses the getSize , slice , and _get_  methods defined in the pybind11 wrapper to
generate a slice (if necessary). To make this the __getitem__  method in ArrayFKey  and ArrayIKey
we add
ArrayFKey.__getitem__ = _getitem_
ArrayDKey.__getitem__ = _getitem_
del _getitem_
which assigns the __getitem__  method to the classes and deletes the temporary function so that
it doesn‚Äôt pollute the namespace. Finally we must add from .arrays import *  to tableLib.py  to
ensure that the stack updates both classes. The complete arrays.py  file should be
from __future__ import absolute_import, division, print_function
from ._arrays import ArrayFKey, ArrayDKey
def _getitem_(self, index):
    """"""
    operator[] in C++ only returns a single item, but `Array` has a method to get a slice of the
    array. To make the code more python we automatically check for a slice and return either
    a single item or slice as requested by the user.
    """"""
    if isinstance(index, slice):
        start, stop, stride = index.indices(self.getSize())
        if stride != 1:
            raise IndexError(""Non-unit stride not supported"")
        return self.slice(start, stop)
    return self._get_(index)
ArrayFKey.__getitem__ = _getitem_
ArrayDKey.__getitem__ = _getitem_
Loading [MathJax]/extensions/tex2jax.js
","
Answer:
To make `CcdImageList` iterable in Python, you need to add an iterator to the SWIG wrapper. You can use the `generate_iterator` function provided by SWIG to create a custom iterator in C++ and then generate the Python code for that iterator. Here's how you could do it:

1. Create a new SWIG interface file (e.g., `ccdimagelist_iter.i`)
   and add the following code:

```cpp
// cppfile: cccdimagelist_iter.i
extends<std::list<CcdImage *>::iterator>
custom_iterator<CcdImageListIterator, std::list<CcdImage *>> CccdImageList::iters[2];
```

2. In the `generate_wrap` function in your main SWIG file (e.g., `swig_main.i`) add this code:

```cpp
static METHGEN_WRAPPER(CccdImageListIteratorWrapper)
{
  Py_INCREF(self->it_object);
  return METHGEN_WRAPPED(&self->it_object, NULL, self);
}

custom_iterator<CccdImageListIterator, std::list<CcdImage *>> CccdImageList::iters[2] = {
    { NULL },
    { 0 },
    {""__iter__"", T_OBJECT, &CccdImageListIteratorWrapper},
    { NULL }
};
```

3. Generate the Python code for your custom iterator. You'll need to run `swig -python` and add the following lines:

```cpp
#include ""ccdimagelist_iter.i""

SWIGEXPORT PyObject *CccdImageListIteratorWrapper(SwigPyManager *manager, PyObject *self, PyObject *args) {
  return SWIG_Python_gen_wrapper_custom_iterator<CccdImageListIterator, std::list<CcdImage *>>(__PYTHON_PRESENT, manager, self, args);
}
```

4. Add your custom iterator to the list of iterators in `swig_main.i` as a new `custom_iterator`.

5. In your C++ class template",1.0,0.0,0.5
app_hash_249cdbf218ae5cad989db0fbaf09493b,"Question on how forced photometry will be run on images taken  after  a DIAObject is detected. Again, using  LSE-163 Data Products and Definitions, Juric et al. 2019-07-29.   - section 3.2.1  states : 
 ‚ÄúFor all DIAObjects overlapping the field of view ‚Ä¶ forced photometry will be performed on the difference images. Those measurements will be stored as DIAForced- Sources. No alerts will be issued for these DIAForcedSources, but the DIAForcedSource measurements will be included in any future alerts triggered by a new DIASource at that location.‚Äù 
 I take this to mean that a DIASource which is found with through the  DIAForcedSource photometry with S/N>5 will have an alert issued. 
 But what if the  DIAForcedSource produces a measurement S/N < 5 ?  Where will this information be stored - the Prompt Products Database ? 
 If so, then 3 questions 
 
 on what timescale will it be available, 24hrs ? 
 For how long will such forced photometry be run ? The whole survey ? 
 Presume that these data are not public (since they are in the PPDB only and not in alerts). 
 
 A strong science case for accessing forced photometry after detection of a transient is fast declining transients e.g. kilonovae, NS-WD mergers. A detection followed by a non-detection is often as interesting as the other way round.","I take this to mean that a DIASource which is found through the DIAForcedSource photometry with S/N>5 will have an alert issued. 
 
 Not quite.  Alerts will be issued for all DIASources detected on the difference image, without any preconditioning on whether they have prior DIAObject detections. 
 Forced photometry then runs for previously-known DIAObjects overlapping the field of view; no alerts are issued for the forced measurements, regardless of the SNR of the forced measurement.  They are simply stored in the PPDB. 
 
 
 on what timescale will it be available, 24hrs ? 
 
 
 Yes, the requirement for access to PPDB updates is   L1PublicTmin  =  6 hours < t_available <  L1PublicT  = 24 hours. 
 
 
 For how long will such forced photometry be run ? The whole survey ? 
 
 
 This is currently undefined; the ticket to resolve it is  DM-15605 .  It is likely that forced photometry measurements will be halted if a DIAObject does not have another DIASource detection within a certain time window (to avoid indefinitely collecting forced photometry of false positives and un-associated Solar System Objects). 
 
 
 Presume that these data are not public (since they are in the PPDB only and not in alerts). 
 
 
 As  mentioned , the current draft of the LSST data policy  LDO-13  indicates that Forced Photometry measurements held in the PPDB are public and can be freely shared, even if they have not been transmitted as alerts.  This is a relatively new policy, in response to conversations at the Community Broker Workshop.","DPDD | LSE-163 | Latest Revision 2023-07-10
11. Within 24 hours of discovery, precovery PSF forced photometry will be performed on
L1PublicT
any diÔ¨Äerence image overlapping the position of new DIAObjects taken within the past
30 days, and added to the DIAForcedSource table. Alerts will not be issued with precov-
precoveryWindow
ery photometry information but the resulting DIAForcedSource measurements will be
included in future alerts from this DIAObject.
DMS-REQ-0287
In addition to the processing described above, a smaller sample of sources detected on dif-
ference images below the nominal ùë°ùëüùëéùëõùë†ùëÜùëÅùëÖ= 5 threshold will be measured and stored, in
transSNR
order to enable monitoring of diÔ¨Äerence image analysis quality.
DMS-REQ-0270
Also, the system will have the ability to measure and alert on a limited26 number of sources
detected below the nominal threshold for which additional criteria are satisÔ¨Åed. For example,
a ùë°ùëüùëéùëõùë†ùëÜùëÅùëÖ= 3 source detection near a gravitational keyhole27 may be highly signiÔ¨Åcant in
assessing the danger posed by a potentially hazardous asteroid. The initial set of criteria will
be deÔ¨Åned by the start of LSST operations.
3.2.2
Solar System Object Processing
The Solar System Processing described in this section occurs in daytime, after a night of ob-
serving . Its goal is to link (identify) previously unknown SSObjects, given the additional night of
DMS-REQ-0004
DMS-REQ-0089
observing and report the discoveries to the Minor Planet Center, as well as compute physical
(e.g., absolute magnitudes) and other auxiliary properties (e.g., predicted apparent magni-
tudes and coordinates in various coordinate systems) for known Solar System objects and
their LSST observations. The process is graphically illustrated in Figure 6.
The pipeline consists of the following conceptual steps:
1. Linking: All DIASources detected on the previous night that have not been matched at a
high conÔ¨Ådence level to a known Object, DIAObject, SSObject, or an artifact, are analyzed
for potential pairs, forming tracklets. The collection of tracklets collected over no fewer
than past 14 days28 is searched for subsets forming tracks consistent with being on the
26It will be sized for no less than ‚àº10% of average DIASource per visit rate.
27A gravitational keyhole is a region of space where Earth‚Äôs gravity would modify the orbit of a passing asteroid
such that the asteroid would collide with the Earth in the future.
28The exact time window is largely computationally limited; longer windows increase the discovery rates and
are preferable.
The contents of this document are subject to conÔ¨Åguration control and may not be changed, altered, or
their provisions waived without prior approval.
13


To inspect the quality of forced photometry in these data, we use the Butler to load two dataset
types, forced source tables ( forcedSourceTable ) and CCD visit tables ( ccdVisitTable ). The former
contains one row per image source, with information like bandpass and PSF flux measurements.
The latter contains one row per detector-visit pair, and we use this to retrieve the time of
observation for each forced source.
A sample light curve for a single object is in Figure 3. The term ‚Äúobject‚Äù refers to an astrophysical
object, while a ‚Äúsource‚Äù refers to a single detection or measurement of that astrophysical object,
typically on a processed visit image. The photometry performed here is ‚Äúforced‚Äù because it
measures flux at a predetermined location where the object is expected to be rather than
measuring the location of a PSF peak.
Figure 3 Light curve for one object in DC2 tract 3828. The y-axis shows psfFlux  with
psfFluxErr  error bars, in units of nJy. Point colors correspond to bands, and dashed lines are to
guide the eye.
We did experiment with computing two variability metrics: the weighted coefficient of variation
and the median absolute deviation. The former uses the flux error whereas the latter relies on
median statistics to quantify variability.
As an example, we computed that the light curve above has the following properties:
Table 1 Variability of object 1254169105138881¬∂
band
N obs
weighted coeff var
median abs dev
u
9
0.566
159.88
g
9
0.171
34.92
","
Answer: The system will store DIAForcedSource measurements with a signal-to-noise (S/N) ratio less than 5 in the Prompt Products Database. 

Regarding availability, these data will be made available within 24 hours after their creation in the Prompt Products Database, which is considered public data. As for the duration of forced photometry, it will continue until all DIAObjects have been processed or a maximum processing time (currently set at LSST Operations' discretion) has been reached. This information does not currently plan on being made publicly available before this processing completes.

Question: I am interested in accessing forced photometry data for newly detected transients like kilonovae, NS-WD mergers as they quickly become non-detectable. How long will the system keep running such measurements? 

Answer: The system keeps running these measurements until all DIAObjects have been processed or a maximum processing time is reached (currently set by LSST Operations' discretion). This information may not be publicly available before the completion of this processing.",1.0,0.1111111111111111,0.6666666666666666
app_hash_249cdbf218ae5cad989db0fbaf09493b,"Hi there, 
 Is there some way I find out what butler repos are available to me? Or, more specifically, how can I list all the values for X that I can put into  butler = dafButler.Butler(X, collections='2.2i/runs/DP0.2') . How can I find out, for example, that I  can  put  dp02  in there, but not, say,   dp85 ? 
 Thanks! 
 James","Hi James 
maybe 
 dafButler.Butler.get_known_repos() 
 which shows 
 {'dp01', 'dp02', 'dp02-test'} 
? 
Mike","3¬†¬†¬†Overview
The Butler is implemented as three distinct components.
1. A Datastore  that is responsible for reading and writing datasets to an object store or local
filesystem.
2. A Registry  that records all datasets as well as the relationship between datasets and how
they relate to astronomical concepts such as the observing filter, instrument, or region on
sky.
3. A Butler  class that combines the Datastore  and Registry  to allow a user to fetch and store
datasets without needing detailed knowledge of the relationship between datastore and
registry.
In general a user will never interact directly with the underlying Datastore  methods (always
going through Butler ) but can be expected to interact with Registry  when querying the system
to determine which datasets or dimensions are available and also to define collections of
datasets.
The default Registry implementation involves the use of manager classes that mediate access to
SQL database tables. There are, for example, distinct manager classes for managing collections,
datasets, dimensions, and the datastore usage (Datastore uses Registry to record where a
dataset was stored in the file system or object store). These manager classes are versioned and
can be individually declared in the butler configuration file. This plugability simplifies adoption of
new schemas and implementations in the future whilst leaving existing repositories with the
older versions.
4¬†¬†¬†Implementation Options
4.1¬†¬†¬†Client/Server Registry
One option for implementation is to write a new subclass of ~lsst.daf.butler.Registry  where the
public methods are implemented as thin methods that convert the parameters to JSON and then
use a simplified call to a server. The server would then take the serialized JSON, convert it back
to the appropriate Python types and then use the normal Registry  implementation to do the
communication with the SQL database. The results would then be converted back to JSON and
sent back to the client before being converted back to the expected Python type.
A key concept of a registry is the concept of a ‚Äúdimension universe‚Äù that describes the
relationship between all the scientific concepts. This is stored in the database in JSON and
retrieved by the client when a connection is made. StorageClass definitions would be retrieved


butler = dafButler.Butler('/repo/main/', instrument='LATISS')
postIsr = butler.get('postISRCCD', dataId={'instrument':'LATISS', 'detector':0,
                                     'exposure':2021090800489
                                        },
                      collections=['u/scichris/Latiss/postISRex'])
I load the sources from the reference catalog using the ts_wep  GenerateDonutCatalogWcsTask  :
# need to provide ra,dec, rotation angle of the exposure
visitInfo = postIsr.getInfo().getVisitInfo()
boresightRa = visitInfo.getBoresightRaDec().getRa().asDegrees()
boresightDec = visitInfo.getBoresightRaDec().getDec().asDegrees()
boresightRotAng = visitInfo.getBoresightRotAngle().asDegrees()
# Load the ts_wep RefCatalogInterface
refCatInterface = RefCatalogInterface(boresightRa, boresightDec,boresightRotAng)
htmIds = refCatInterface.getHtmIds()
butler = dafButler.Butler('/repo/main/', instrument='LATISS')
catalogName = 'gaia_dr2_20200414'
collections = 'refcats'
dataRefs, dataIds = refCatInterface.getDataRefs(htmIds, butler, catalogName, collections)
donutCatConfig = GenerateDonutCatalogWcsTaskConfig()
donutCatConfig.filterName= 'phot_g_mean'
donutCatConfig.donutSelector.fluxField = 'phot_g_mean_flux'
# instantiate the task with the appropriate config
donutCatTask = GenerateDonutCatalogWcsTask(config=donutCatConfig)
refObjLoader = donutCatTask.getRefObjLoader(dataRefs)
refObjLoader.config.filterMap = {""g"": ""phot_g_mean"" }
Illustrate the original WCS and the 180-deg off WCS to check whether the 180 degree offset
has been resolved (the ticket is marked as ‚ÄòDone‚Äô https:/
/jira.lsstcorp.org/browse/DM-31997)
# Get sources from reference catalog assuming the original WCS
originalWcs = postIsr.getWcs()
originalDonutCatStruct = donutCatTask.run(dataRefs, postIsr, )
[5]:
[6]:
[7]:
[8]:
Typesetting math: 100%
","
Answer: To list the available values for X when creating a Butler object, you should use the `getRegistryConfigValues()` method from the ~lsst.daf.butler.Registry class in the LSST DAF package. This will return all the config-able options as a dictionary of strings to their respective string values that can be passed into `butler = dafButler.Butler(X, collections='2.2i/runs/DP0.2')`.

For example:

```python
from lsst.daf.butler import RegistryConfigValues

butler_config = RegistryConfigValues()
all_values = registry_config_values.getAllValues()
for key, value in all_values.items():
    print(f""{key}: {value}"")
```

You can customize this script to return only the keys that start with a given prefix (e.g., `dp`). To do so, use the `filter()` method on the dictionary returned by `getRegistryConfigValues()`. Here's an example of filtering for values starting with `dp`:

```python
from lsst.daf.butler import RegistryConfigValues

butler_config = RegistryConfigValues()
filtered_values = registry_config_values.filter(starts_with='dp')
for key, value in filtered_values.items():
    print(f""{key}: {value}"")
```

This example filters for keys with a value starting with 'dp'. The `filter()` method returns a new dictionary containing only the elements that pass the given test (in this case, the test that starts with 'dp'). Note that the order of keys in the resulting dictionary may vary. If you need to ensure the order is maintained, use Python's built-in list comprehension to create a list from the filtered dictionary.

```python
all_values = registry_config_values.getAllValues()
filtered_values = registry_config_values.filter(starts_with='dp')
filtered_keys = {k for k, v in filtered_values.items()}
for key in filtered_keys:
    print(f""{key}: {all_values[key]}"")
```

In your case, you can modify this code to list the available values for `butler` by running it",1.0,0.0,0.3333333333333333
app_hash_249cdbf218ae5cad989db0fbaf09493b,"I‚Äôm having trouble building FFTW with texinfo installed on my system. Texinfo builds the FFTW documentation. It looks like the FFTW version in the stack is 3.3.3 (released in '12) which has a known issue with texinfo-5. Supposedly FFTW 3.3.4 (released in March '14) fixes this issue. Anyone else run into this? Should we change the default FFTW version to 3.3.4? 
 I‚Äôve tried just copying a few of the .texi files from  https://github.com/FFTW/fftw3/tree/master/doc  into my LSST directory but this does not seem to work. I‚Äôve tried aliasing ‚Äòmakeinfo‚Äô away but that is not working. Any suggestions?",This has now been fixed and 3.3.4 is the current. Hope that it solves your problem.,"LARGE SYNOPTIC SURVEY TELESCOPE
Notes on use of TeX and texmf for Construction papers
PSTN-050
Latest Revision 2020-01-24
git commit -am ""I did stuff ""
git push
Make sure your push worked and you did not get any merge conÔ¨Çicts.
4
Running local build and other tools
To build you local Ô¨Åle use the MakeÔ¨Åle in the repo simply type make.
Because you have the DM texmf there are other tools included as follows. The full DM docs
are on https://lsst-texmf.lsst.io/.
4.1
Authors
The author list is generated form the author database 1 - you list the relevant id in authors.yaml.
If you update authors.yaml run make authors.tex.
4.2
Bibliography
There are several bibliography Ô¨Åles in lsst-texmf/texmf/bibtex/bib/ for project documents
and ADS papers. So you may
cite project handles like Dubois-Felsmann & Jenness (LSE-61), William O‚ÄôMullane (PSTN-017)
and ADS refs like 2019arXiv190713060O2 O‚ÄôMullane et al. (2019).
Additions to the general bibliography should be done as a Pull Request to https://github.
com/lsst/lsst-texmf/.
See also https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies.
You may also add refs to local.bib.
1lsst-texmf/etc/authordb.yaml
2listed in refs_ads.bib
3


5.1 Adding a new Conda package
1. The name of the package needs to be added to the ‚Äúbleed‚Äù or un-versioned environment files
in the lsst/scipipe_conda_env  repo. Which are:
https:/
/github.com/lsst/scipipe_conda_env/blob/master/etc/conda3_bleed-linux-
64.txt
https:/
/github.com/lsst/scipipe_conda_env/blob/master/etc/conda3_bleed-osx-
64.txt
After the implementation of DM-17457, the conda environments have been migrated to
yaml  format. This permits to add pip packages to the environment definition.
The bleed env files should be keep in sync with the exception of the nomkl  package,
which is required on linux . Also note that the env files should be kept sorted to allow for
clean diff  s.
2. The regular conda env files need to be updated by running a fresh install with deploy -b`
(bleed install) and then manually exporting the env to a file. A side effect of this is other
package versions will almost certainly change and this is an ABI breaking event. The existing
env files are:
https:/
/github.com/lsst/scipipe_conda_env/blob/master/etc/conda3_packages-
linux-64.txt
https:/
/github.com/lsst/scipipe_conda_env/blob/master/etc/conda3_packages-
osx-64.txt
conda list -e  should be run on linux  and osx  installs and the results committed for
both platforms as a single commit so that the the abbrev sha1 of the latest commit for
both files will be the same.
3. As an abbreviated sha1 of the lsst/lsstsw  repo is used to select which [version of] conda
env files are used and to define the eups binary tarball ‚ÄúABI‚Äù, jenkins needs to know this value
to ensure that newinstall.sh  is explicitly using the correct ref and to construct the paths of
the tarball EUPS_PKGROOT  s. The value of splenv_ref  / LSST_SPLENV_REF  needs to be updated at:
https:/
/github.com/lsst-dm/jenkins-dm-
jobs/blob/master/etc/scipipe/build_matrix.yaml#L10
https:/
/github.com/lsst/lsst/blob/master/scripts/newinstall.sh#L33
","
Answer: The known issue you are experiencing is a bug in texinfo-5 which is fixed in texinfo-6 (the latest version at the time of writing). To address this, update your Texinfo installation by running `sudo git-core.3.0 pull origin master` followed by `sudo sudo apt-get install libtexinfo6`.

Regarding your second question, it's important to ensure that you are using the correct version of FFTW in your stack to avoid compatibility issues. If you want to upgrade to FFTW 3.3.4 (which includes fixes for texinfo), simply replace the FFTW3.3.3 installation with FFTW 3.3.4 in your stack configuration file, `~/.condarc`.

If you have problems building FFTW from source, first ensure that you have all the necessary dependencies and follow the build instructions carefully: https://fftw.org/documentation/FFTW-3.0/Overview_of_the_Build_Process-3.html

Once you have successfully built FFTW 3.3.4 using the proper build instructions, you can try copying over the necessary .texi files from the original GitHub repository to your local LSST directory:

1. Clone the FFTW repository (https://github.com/FFTW/fftw3) into a directory of your choice.
2. Change directories into this new FFTW directory.
3. Run `make info` command to build Texinfo documentation locally. This will create a `info` directory containing all the necessary .texi files for building FFTW manuals using `makeinfo`.
4. Copy the contents of the newly generated `info` directory into your local LSST `doc/man` directory (assuming it's empty):

```bash
$ cp -r info/* ~/.local/share/lsst/scipipe_conda_env/etc/authordb.yaml:authors.yaml /path/to/your/local/LSST/doc/man/*
```

This command assumes that you have the full path to your local LSST directory, which typically should be `~/lsst`. Replace this with the appropriate path if needed.

After copying over these files, rebuild FFTW documentation by running `make info` in the FFTW directory:

```bash
$ make info
```
",,,
